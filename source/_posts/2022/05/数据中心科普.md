---
title: 数据中心科普
date: 2022-05-09 20:46:40
tags: [科普, 数据中心]
categories: 资料
description:
---

&emsp;&emsp;&emsp;&emsp;数据中心，英文缩写叫 IDC，也就是 Internet Data Center（互联网数据中心）。

![01](/pictures/post/2022/05/01.png)


&emsp;&emsp;&emsp;&emsp;从作用上来看，**数据中心就是一个超大号的机房**，里面有很多很多的服务器，专门对数据进行集中管理（存储、计算、交换）。

&emsp;&emsp;&emsp;&emsp;根据业界机构统计，2020 年全球经数据中心处理的数据流量高达 15.3ZB（1ZB≈10 亿 TB），占全球总流量的 99.35%。也就是说，几乎所有的互联网数据，都离不开数据中心的处理，由此可见其重要性。
<!--more-->
&emsp;&emsp;&emsp;&emsp;按现在比较流行的说法，数据中心是像水厂、电厂一样的重要基础设施，是数字经济的动力引擎，也是国家和社会发展的支撑底座。


# 数据中心的发展阶段

&emsp;&emsp;&emsp;&emsp;我们先来看看数据中心的发展历史。

&emsp;&emsp;&emsp;&emsp;上世纪 60 年代的时候，人类还处于大型机时代。那时，为了存放计算机系统、存储系统和电力设备，人们修建了机房，并将其称为“**服务器农场**”（Server farm）。

&emsp;&emsp;&emsp;&emsp;这个“服务器农场”，被认为是数据中心的最早原型。

&emsp;&emsp;&emsp;&emsp;到了 90 年代，随着互联网的诞生和蓬勃发展，很多公司开始推行信息化。他们建设自己的网站，还搭建了大量的邮件、FTP、OA 办公自动化等服务器。

&emsp;&emsp;&emsp;&emsp;有些公司将服务器放在企业内部的机房。也有些公司，因为服务器不多，但又不愿意放在办公室（噪音大、容易断电、安全性低），于是，**就“托管”在运营商机房**，租用运营商的场地、电力、网络带宽，让对方代为管理和维护。

![02](/pictures/post/2022/05/02.png)

<center>▲ 数据中心的早期阶段（第一阶段）</center>

&emsp;&emsp;&emsp;&emsp;于是，数据中心的概念开始逐渐形成。1996 年。一家名叫 Exodus 的美国公司（专门从事机房设施建设和带宽服务），最早提出了“IDC”这个叫法。

这就是 IDC 数据中心发展的早期阶段。

&emsp;&emsp;&emsp;&emsp;1997 年，苹果公司推出了一款名叫“Virtual PC（虚拟 PC）”的虚拟机软件。后来，VMWare 也推出了现在大名鼎鼎的 VMWare Workstation，标志着虚拟机时代的到来，为数据中心的演进打下了基础。

&emsp;&emsp;&emsp;&emsp;随着时间推移，第一代数据中心的托管服务开始精细化，从完整的服务器主机托管，延伸出了网站托管，出现了**虚拟主机服务**。

&emsp;&emsp;&emsp;&emsp;也就是说，在某一台服务器上，通过虚拟主机软件，虚拟出 N 个网站主机，出租给 N 个客户使用。除了网站之外，还出现了数据存储空间租用等多样化的服务。这就是 IDC 数据中心的第二个阶段。

![03](/pictures/post/2022/05/03.png)

<center>▲ 数据中心的第二阶段</center>

&emsp;&emsp;&emsp;&emsp;再往后，到了 21 世纪初，亚马逊、谷歌等公司提出了云计算，从而将数据中心带入了第三个阶段（**云计算阶段**），持续至今。

&emsp;&emsp;&emsp;&emsp;云计算阶段，是第二阶段的升级演进。它通过虚拟化技术、容器技术，彻底实现了数据中心服务器算力资源的池化。所有的 CPU、内存、硬盘等资源，都由更为强大的虚拟化软件管理，然后分配给用户使用。

&emsp;&emsp;&emsp;&emsp;从物理硬件出租，进化为虚拟硬件出租，甚至软件平台出租、服务出租。IaaS、PaaS、SaaS，就这样出现在我们面前。

![04](/pictures/post/2022/05/04.png)

<center>▲ 数据中心的第三阶段（云计算阶段）</center>

# 数据中心的组成结构

&emsp;&emsp;&emsp;&emsp;接下来，我们再看看数据中心到底由哪些部分组成。

&emsp;&emsp;&emsp;&emsp;前面说了，数据中心就是一个大机房。所以，从硬件种类上来说，数据中心和我们以前经常看到的企业内部机房差不多，只不过规格、档次和管理级别更 high level 一点。

![05](/pictures/post/2022/05/05.jpg)

<center>▲ 数据中心内部</center>

整体来看，数据中心的硬件分为两类，分别是**主设备和配套设备**。

- 主设备，是真正实现计算和通信功能的设备，也就是以服务器、存储为代表的 IT 算力设备，以及以交换机、路由器、防火墙为代表的通信设备。
- 配套设备，则是为了保证主设备正常运转而存在的底层基础支撑设备（也包括一些设施）。

底层基础支撑设备设施，又分为多种，主要是供配电系统和散热制冷系统，另外还有消防系统、监控系统、楼宇管理系统等。

![06](/pictures/post/2022/05/06.png)

## 主设备

&emsp;&emsp;&emsp;&emsp;我们先看看主设备。

&emsp;&emsp;&emsp;&emsp;数据中心最基础的主设备，当然是服务器。服务器其实也就是个高性能计算机，大家应该都见过，里面和台式机一样，是 CPU、内存、主板、硬盘、显卡（GPU）、电源等。

![07](/pictures/post/2022/05/07.png)

<center>▲ 某型号服务器正面视角</center>

&emsp;&emsp;&emsp;&emsp;以前，服务器基本上都是 Intel 架构（更早的时候，还有 PowerPC、SPARC 等）。如今，随着国家政策变化，国产 CPU 崛起，占据越来越多的份额。这些国产 CPU 采用 ARM 架构，性价比更高，成本更低。

![08](/pictures/post/2022/05/08.jpeg)

<center>▲ 华为芯片 Hi1620（数据中心用 ARM 架构处理器）</center>

&emsp;&emsp;&emsp;&emsp;服务器，一般都摆放在机架（也叫机柜）上。

![09](/pictures/post/2022/05/09.jpg)

<center>▲ 服务器机架</center>

&emsp;&emsp;&emsp;&emsp;一个常见标准机架，高度尺寸通常是 42U。U 是一种表示服务器外部尺寸的单位，是 unit 的缩略语，**1U 等于 4.445cm**。机架宽度的话，有 600mm 或 800mm。

&emsp;&emsp;&emsp;&emsp;机架的深度有很多种，包括 600mm、800mm、900mm、1000mm、1200mm 等。通常来说，IT 设备（服务器）机架的深度更深（1100mm 或 1200mm），而通信设备的深度会浅一些（600mm）。

![10](/pictures/post/2022/05/10.png)

<center>▲ IT 设备和通信设备的机架深度对比</center>


&emsp;&emsp;&emsp;&emsp;机架里面的 IT 设备，除了服务器之外，还有磁盘阵列这样的专业存储设备。

&emsp;&emsp;&emsp;&emsp;现在到处都讲大数据，我们人类产生的数据量每年都在激增，也就增加了对存储设备的数量和性能要求。

&emsp;&emsp;&emsp;&emsp;大家应该都知道，现在主流的计算机存储硬盘分为 HDD 和 SSD 两种。HDD 就是我们传统的机械硬盘，而 SSD 是逐渐开始普及的固态硬盘。SSD 属于半导体存储器，存储速率快，体积小，非常受欢迎。但是，它的价格昂贵。对于数据中心来说，出于性价比考虑，HDD 仍然是主流选择。而 SSD，目前主要用于高端客户、高性能需求业务。

&emsp;&emsp;&emsp;&emsp;除了 IT 算力设备之外，就是交换机、路由器、防火墙等数据通信设备了。

&emsp;&emsp;&emsp;&emsp;说到交换机，就要提到一个名词 ——**TOR，Top of Rack**。

&emsp;&emsp;&emsp;&emsp;TOR 交换机，是数据中心领域的常见名词。顾名思义，就是机架顶部交换机的意思。这类交换机，是数据中心最底层的网络交换设备，负责连接本机架内部的服务器，以及与上层交换机相连。

![11](/pictures/post/2022/05/11.jpeg)

<center>▲ TOR 交换机的位置</center>

&emsp;&emsp;&emsp;&emsp;事实上，机架交换机并没有说一定要放在机架顶部。它既可以在机架顶部，也可以在机架的中部或底部。之所以通常放在顶部，只是因为这样最有利于内部布线。

&emsp;&emsp;&emsp;&emsp;机架再往上，就是一排机架、N 排机架。将这些机架和服务器连接起来，就需要数据中心组网技术。

![12](/pictures/post/2022/05/12.jpg)

&emsp;&emsp;&emsp;&emsp;现在最流行的数据中心组网架构，就是叶脊网络（Spine-Leaf）

![13](/pictures/post/2022/05/13.jpeg)

<center>▲ 叶脊网络架构</center>

&emsp;&emsp;&emsp;&emsp;值得一提的是，现在数据中心为了高带宽传输数据，**普遍使用光纤替代网线**。所以，光纤、光模块和光通信设备（OTN 等），成为数据中心重要的组成部分。

&emsp;&emsp;&emsp;&emsp;尤其是光模块，高速率光模块（例如 400G）价格很昂贵，占了数据中心很大一块成本，制约了发展。

![14](/pictures/post/2022/05/14.png)

<center>▲ 光模块</center>

&emsp;&emsp;&emsp;&emsp;现在还有一个比较流行的光通信名词，叫做 DCI，也就是 Data Center Inter-connect（**数据中心互联**）。现在流行分布式部署，数据中心之间的数据流量很大，对带宽要求很高。

&emsp;&emsp;&emsp;&emsp;所以，运营商和云服务商就搞 DCI，建设数据中心之间专门的光通信骨干网，是很大一块市场。

&emsp;&emsp;&emsp;&emsp;我们国家搞的那个“东数西算”，就涉及到数据中心的互联互通，对 DCI 相关市场有不可忽视的刺激作用。

![15](/pictures/post/2022/05/15.png)

<center>▲ 光通信骨干网设备</center>

## 配套设备

&emsp;&emsp;&emsp;&emsp;接下来，我们看看数据中心的配套支撑设备和设施。

&emsp;&emsp;&emsp;&emsp;先看供配电。供电是数据中心正常运作的基础。没有电，数据中心就是废铁。

&emsp;&emsp;&emsp;&emsp;数据中心的配电设备，主要作用就是电能的通断、控制和保护。最主要的配电设备，就是**配电柜**。

&emsp;&emsp;&emsp;&emsp;数据中心配电柜分为中压配电柜和低压配电柜。中压配电柜主要是 10kV 电压等级，向上接入市电，向下接低压配电柜。低压配电柜主要是 400V 电压等级，对电能进行进一步的转换、分配、控制、保护和监测。

![16](/pictures/post/2022/05/16.jpg)

<center>▲ 数据中心供配电示意图（案例）</center>

![17](/pictures/post/2022/05/17.jpeg)

<center>▲ 数据中心低压配电柜（图片来自网络）</center>

&emsp;&emsp;&emsp;&emsp;除了配电柜之后，为了保证紧急情况下的正常供电，数据中心还会配备大量的 UPS（不间断电源）甚至柴油发电机组。

&emsp;&emsp;&emsp;&emsp;“UPS + 市电”是传统的供电方案。现在，更流行的是“**HVDC + 市电**”的方案。

&emsp;&emsp;&emsp;&emsp;HVDC 是 High Voltage Direct Current，高压直流输电。它和 UPS 之间的区别涉及到较为复杂的强电知识，后续有机会再专门介绍。

&emsp;&emsp;&emsp;&emsp;总之，“HVDC + 市电”的可靠性和安全性更高，供电效率强于“UPS + 市电”，是不间断电源的主流发展趋势。

&emsp;&emsp;&emsp;&emsp;我们简单说说-48V 和 220V。有过 ICT 行业实际从业经验的同学们都知道，服务器这样的 IT 设备通常是使用 220V 交流电，而核心网、无线等通信设备则更多使用的是-48V 直流电。

&emsp;&emsp;&emsp;&emsp;市电供电，一般都是交流。数据中心，一般既会提供-48V 直流，也会提供 220V 交流（通过 AC-DC 转换和 DC-AC 逆变转换）。

&emsp;&emsp;&emsp;&emsp;事实上，**直流现在正成为更多数据中心的选择**（例如谷歌），因为直流的损耗更小，对电能的利用率更高，符合现在数据中心高算力下的高能耗发展趋势。

&emsp;&emsp;&emsp;&emsp;再来看看散热制冷。

&emsp;&emsp;&emsp;&emsp;制冷系统是数据中心除了主设备之外的第二大耗能主体。目前，数据中心制冷主要包括两种方式，**一种是风冷，另一种是液冷**。

&emsp;&emsp;&emsp;&emsp;风冷一般采用风冷空调系统。和我们家用空调一样，数据中心风冷空调也分为室内机和室外机。相对来说，技术成熟，结构简单，容易维护。

&emsp;&emsp;&emsp;&emsp;液冷，是采用液体作为冷媒，进行降温散热。液体的导热能力是空气的 25 倍，相同体积下，液体带走的热量是空气的近 3000 倍。从噪音角度来看，同等散热水平下，液冷的噪音比风冷降低 20-35 分贝。从能耗的角度来看，液冷比风冷节约电量 30%-50%。

&emsp;&emsp;&emsp;&emsp;目前，液冷技术被行业普遍看好，但仍处于探索阶段。从总体来看，液冷的市场前景非常广阔，据称市场规模超过千亿。

&emsp;&emsp;&emsp;&emsp;关于制冷和散热，值得一提的是，机柜池级、排级和机柜级等近端制冷方式，正在崛起，成为新建数据中心的主流选择。

&emsp;&emsp;&emsp;&emsp;传统的制冷都是房间级，对整个机房进行空调制冷，这种方式制冷路径太长，效率太低，无法满足高功耗设备的散热需求，能耗也很大。

&emsp;&emsp;&emsp;&emsp;**机柜池级、排级和机柜级**，是以一个机柜池、一排机柜或者单个机柜为中心，进行散热设计。

![18](/pictures/post/2022/05/18.png)

&emsp;&emsp;&emsp;&emsp;机柜排级散热，以一排机柜为对象，进行风道设计。这种方式，气流路径明显缩短，散热效率很高。

&emsp;&emsp;&emsp;&emsp;除了配电和散热制冷之外，数据中心还有一些和管理运维有关的设备设施，例如动环监控系统、楼宇自动控制系统、消防系统等。

&emsp;&emsp;&emsp;&emsp;动环监控，就是动力和环境监控，实时监测和管理数据中心运行状态的。

&emsp;&emsp;&emsp;&emsp;这些年，在传统动环监控系统的基础上，开始演进出了 DCIM。

&emsp;&emsp;&emsp;&emsp;DCIM 全名叫做 Data Center Infrastructure Management，是知名咨询公司 Garter 提出来的。它的管理范围更为全面，采用工具监控、管理和控制数据中心的所有 IT 主设备以及配套基础设施。

![19](/pictures/post/2022/05/19.jpg)

&emsp;&emsp;&emsp;&emsp;数据中心的消防系统比较有意思。因为机房里都是电子设备，所以发生火灾的话，肯定不能直接喷水、泡沫或粉尘。

&emsp;&emsp;&emsp;&emsp;那怎么办呢？**气体灭火**。

&emsp;&emsp;&emsp;&emsp;出现火灾后，火灾和烟雾传感器的警铃响起，然后，机房区域可以释放氩气、氮气这样的惰性气体，剥夺火焰中的氧气，实现灭火（大约几十秒就能搞定）。

![20](/pictures/post/2022/05/20.png)

<center>▲ 气体灭火（图片来自网络）</center>

## 模块化数据中心

&emsp;&emsp;&emsp;&emsp;数据中心是一个庞大的系统，建设过程非常复杂繁琐，工期也很长。这些年，为了更快速、更灵活地部署数据中心，厂商推出了模块化数据中心的概念。

&emsp;&emsp;&emsp;&emsp;说白了，就是将数据中心的结构系统、供配电系统、暖通系统、消防系统、照明系统、综合布线等进行集成，变成一个一个的“积木”。然后，将“积木”运送到现场后，进行简单吊装、搭建，就可以完成建设和部署。

&emsp;&emsp;&emsp;&emsp;采用这种方式，大型数据中心的建设周期从 18-24 个月压缩成了 6 个月左右，经济效益明显。

![21](/pictures/post/2022/05/21.png)

<center>▲ 模块化数据中心（模型）</center>

# 结语

&emsp;&emsp;&emsp;&emsp;好了，以上就是关于数据中心的介绍。

&emsp;&emsp;&emsp;&emsp;正如我前文所说，数据中心是数字时代重要的信息化基础实施，是算力的重要载体，直接决定了国家的数字竞争力。

&emsp;&emsp;&emsp;&emsp;在“东数西算”战略的带动下，国内数据中心将迎来又一波发展热潮。根据数据统计，2022 年我国数据中心业务市场规模将达到 3200.5 亿元，年均复合增长率高达 27.0%。2025 年，国内数据中心 IT 投资规模有望达到 7070.9 亿元。

&emsp;&emsp;&emsp;&emsp;除了数量激增之外，数据中心正在向绿色化、智能化的方向发展，积极引入 AI 人工智能，提升能效，降低运营复杂度。

未来，数据中心是否会出现新的形态变化，让我们拭目以待吧！